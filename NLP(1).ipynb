{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "979b1b05-f61e-47d8-9905-0f9918571ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6186114-ed09-464f-9f74-952297f6ec3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11de0135-5baf-43dc-bd11-6410a0a53ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59910d67-a73f-41c0-b2fe-ee21361c441c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pizza\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "text = \"pizzas\"\n",
    "res = lemmatizer.lemmatize(text)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c5cb5c-555f-4ae2-b299-76ad0a04dc41",
   "metadata": {},
   "source": [
    "## StopWord Removal\n",
    "- removing less priority words\n",
    "- get faster accurate results\n",
    "#### Differences b/w normalization and stopword\n",
    "- only brings to root but in stop word it reduces the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aa6cc0-9884-4c24-949e-636f722dfbdc",
   "metadata": {},
   "source": [
    "# StopWord Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61982e4c-a9a7-4c54-8426-8bbd2860b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11001914-7d8d-4600-8cf5-889e5e5b36cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79de4c90-5c8b-40ef-a2db-71216ae7bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = stopwords.words('English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "083ce0fa-cdd0-4a10-9acf-7931fd686695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "text = [\"This\",\"is\",\"a\",\"sample\",\"sentence\"]\n",
    "res = [word for word in text if word.lower() not in words]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d457ec90-3c02-45a8-8afe-a84542c97d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "words = set(stopwords.words('english'))\n",
    "text = \"This is a sample sentence\"\n",
    "res = [word for word in text.split() if word.lower() not in words]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e1373-b1f1-45c0-b58e-b17ec5d24473",
   "metadata": {},
   "source": [
    "# 29-01-26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074d153-715a-473f-ae32-cc3baad175c0",
   "metadata": {},
   "source": [
    "# Edit Distance\n",
    "- minimun number of operations\n",
    "- levenstine distance has three types:\n",
    "    1. INsertion\n",
    "    2. Deletion\n",
    "    3. Substitution\n",
    "- Damerau Distance(extenstion of levenstain distance)\n",
    "    1. Transposition(swapping of two positions)-it has one extra root\n",
    "\n",
    "\n",
    "## Project-1:\n",
    "1. Calculate the levenstain distance between START and STARE(matrics operaton postword lemmatization)\n",
    "2. Clean a dirty sentence for a search engine(the input is[\"The quick BROWN foxes...they are JUPING over 10 lazy dogs!\"]\n",
    "3. Build a simple logic based on:\n",
    "   - create a list three spam words eg.,[\"will\",\"cash\",\"free\",\"prize\"] i/p:\"You are WINNING a free prize now\" o/p: in boolean format weather the spam word is present or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07ab9af6-0aa2-4a16-a501-a302277d7173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a9aef8c-c409-4f7d-b70e-99bbf91bc618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'fox', 'jump', 'lazi', 'dog']\n"
     ]
    }
   ],
   "source": [
    "## Project-1\n",
    "#2.Answer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lem = PorterStemmer()\n",
    "text = \"The quick BROWN foxes...they are JUMPING over 10 lazy dogs!\"\n",
    "cleaned_text = re.sub(r'[^a-zA-Z\\s]',' ',text)\n",
    "cleaned_text = cleaned_text.lower()\n",
    "tokens = cleaned_text.split()\n",
    "res1 = [lem.stem(word) for word in tokens]\n",
    "res = [word for word in res1 if word not in stop_words]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dd5ee122-afbb-454c-a1d5-0fc0784f9955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Project-1\n",
    "#3.Answer\n",
    "spam = [\"will\",\"cash\",\"free\",\"prize\"]\n",
    "text = \"You are WINNING a free prize now\"\n",
    "spam_check = any(word in text for word in spam)\n",
    "spam_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cb1437-e5a9-426f-9fd7-3666633edd38",
   "metadata": {},
   "source": [
    "# 31-01-26\n",
    "\n",
    "## Projrct-2\n",
    "1. Prepare a real world messy data for a sentiment analysis model.\n",
    "    - I/p: {A:\"I Love this movie!!! #Awesome #friday\",B:\"I love this movie but the seats were bad\", c:\"LOVED it! Best Movie Ever\".\n",
    "2. understand why keyboard layout matters in NLP a user mean to type \"HELLO\" they acidentally types \"HELLP\" and \"HELLZ\" calculate the standard levensitnie distance for both and also apply a edit distance which word should that suggest as for the correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352f40d6-1e3a-4874-98a3-31fa17e5ab34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
